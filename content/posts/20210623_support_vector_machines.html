---
title: 支持向量机（SVM）原理
author: Fang Yongchao
date: "2021-06-23"
categories:
  - "机器学习"
output:
  blogdown::html_page:
    toc: yes
    toc_depth: 2
draft: true
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#硬间隔和软间隔">硬间隔和软间隔</a></li>
<li><a href="#核函数">核函数</a></li>
<li><a href="#算法原理">算法原理</a></li>
</ul>
</div>

<p>对于线性分类问题，若数据是线性可分的，支持向量机计算支持向量使得硬间隔最大化；当数据是线性不可分时，支持向量机引入松弛变量，计算支持向量使的软间隔最大化。</p>
<p>对于非线性分类问题，支持向量机引入核函数后求解支持向量。</p>
<div id="硬间隔和软间隔" class="section level2">
<h2>硬间隔和软间隔</h2>
<hr />
<p>对于线性可分的训练集，支持向量机寻找一个超平面（图中的实线部分，可以表示为<span class="math inline">\(\omega^*x+b^*=0\)</span>），使得训练集距离该超平面距离的绝对值的最小值最大（表示为<span class="math inline">\(\gamma\)</span>），这意味着该超平面以尽可能大的确信度将数据分成了两个类别。</p>
<p>距离超平面的距离为<span class="math inline">\(\gamma\)</span>的两个超平面（图中的虚线部分）上的样本（红色标记样本）被称为<strong>支持向量</strong>。</p>
<p>显然，如果将样本的实际类别表示为±1，那么样本<span class="math inline">\(i\)</span>到超平面的距离的绝对值可以表示为<span class="math inline">\(\gamma_i = y_i(w^*x_i+b^*)\)</span></p>
<p><img src="/posts/20210623_support_vector_machines_files/figure-html/unnamed-chunk-1-1.png" width="384" style="display: block; margin: auto;" /></p>
</div>
<div id="核函数" class="section level2">
<h2>核函数</h2>
<hr />
</div>
<div id="算法原理" class="section level2">
<h2>算法原理</h2>
<hr />
</div>
