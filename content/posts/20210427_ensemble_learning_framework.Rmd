---
title: 集成学习框架
author: Fang Yongchao
date: "2021-04-27"
output:
  blogdown::html_page:
    toc: yes
    toc_depth: 2
---

## Bagging

----------------------

Bagging，全称为Bootstrap Aggregating，是对训练集进行有放回的抽样得到其子集，然后每个基学习器使用不同的子集进行训练得到预测结果，最终基于所有的基学习器的预测结果进行投票，从而得到集成模型的预测结果。Bagging中的各个基学习器是相互独立的。

随机森林模型即为最常见的Bagging模型，它是由多个决策树基学习器集成得到的集成模型。

## Boosting

----------------------

Boosting的训练过程是有序的，每个基学习器会基于前一个基学习器的结果进行学习，最终基于所有的基学习器的预测值计算得到集成模型的预测结果。

常见的Boosting模型有：

### Adaboost

Adaboost，全称为Adaptive boosting，其基本思想为前一个基学习器中分错的样本的权重会得到加强，在权重进行加强后基于所有样本继续训练新的基学习器。循环上述过程直到错误率低于某个值或循环次数达到某个值时停止。

### GBDT

GBDT，全称为Gradient Boosting Decision Tree，其基本思想为以决策树作为基学习器，后一个基学习器以前一个基学习器的残差作为目标值进行训练，最终将所有基学习器的预测值相加作为集成学习模型的预测值。

目前GBDT的主流的工程实现有XGBoost和LightGBM。