---
title: 梯度提升树（GBDT）原理
author: Fang Yongchao
date: "2021-05-07"
categories:
  - "机器学习"
output:
  blogdown::html_page:
    toc: yes
    toc_depth: 2
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#gbdt算法原理">GBDT算法原理</a></li>
<li><a href="#回归问题下的gdbt">回归问题下的GDBT</a></li>
<li><a href="#二分类问题下的gbdt">二分类问题下的GBDT</a></li>
<li><a href="#多分类问题下的gbdt">多分类问题下的GBDT</a></li>
</ul>
</div>

<div id="gbdt算法原理" class="section level2">
<h2>GBDT算法原理</h2>
<hr />
<p>GBDT可以表示为决策树的加法模型，即：
<span class="math display">\[
\hat y_i = \sum_{k=1}^K f_k(x_i)
\]</span>
其中<span class="math inline">\(f\)</span>表示决策树，<span class="math inline">\(K\)</span>为决策树的数量。</p>
<p>通过前向分步骤算法，第t步得到的决策树模型为<span class="math inline">\(f_t(x)\)</span>，有：
<span class="math display">\[
\hat y_i^{(t)} = \hat y_i^{(t-1)} + f_t(x_i)
\]</span>
其目标函数为：
<span class="math display">\[
obj^{(t)} = \sum_{i=1}^N l\left(y_i, \hat y_i^{(t-1)} + f_t(x_i) \right)
\]</span>
将目标函数使用泰勒一阶展开，有：
<span class="math display">\[
obj^{(t)} = \sum_{i=1}^N \left[ l(y_i, \hat y_i^{(t-1)}) + g_i f_t(x_i) \right]
\]</span>
这里的<span class="math inline">\(g_i = \left[ {\partial\ l(y_i, \hat y_i) \over \partial \hat y_i} \right]_{\hat y_i = \hat y_i^{(t-1)}}\)</span>，即损失函数关于<span class="math inline">\(\hat y_i\)</span>求导后带入<span class="math inline">\(\hat y_i = \hat y_i^{(t-1)}\)</span>。</p>
<p>丢掉目标函数中的常数部分，我们要最优化的是下面的部分：
<span class="math display">\[
\sum_{i=1}^N g_if_t(x_i)
\]</span></p>
<p>于是可以运用梯度下降的思想，沿着负梯度方向拟合第t棵决策树<span class="math inline">\(f_t(x)\)</span>，即对于每个样本，<span class="math inline">\(f_t(x)\)</span>需要拟合的值为：
<span class="math display">\[
r_i = - \gamma g_i
\]</span>
其中<span class="math inline">\(\gamma\)</span>即为梯度下降的步长，或者可以称作学习率。通过调整<span class="math inline">\(\gamma\)</span>可以达到<strong>正则化</strong>的目的。</p>
</div>
<div id="回归问题下的gdbt" class="section level2">
<h2>回归问题下的GDBT</h2>
<hr />
<p>在回归问题下，如果使用二分之一的MSE作为损失函数，即：
<span class="math display">\[
l(y, \hat y) = {1 \over 2} (y - \hat y)^2
\]</span>
这时候，第t棵决策树<span class="math inline">\(f_t(x)\)</span>需要拟合的值就是：
<span class="math display">\[
r_i = \gamma\ (y_i - \hat y_i^{(t-1)})
\]</span>
当步长（学习率）为1时，拟合的即为前t-1棵决策树模型之和与真实值的残差。</p>
</div>
<div id="二分类问题下的gbdt" class="section level2">
<h2>二分类问题下的GBDT</h2>
<hr />
<p>在二分类问题下，损失函数为：
<span class="math display">\[
l(y, \hat y) =  y\ln(1 + e^{-\hat y}) + (1-y)\ln(1 + e^{\hat y})
\]</span>
注意这里的<span class="math inline">\(\hat y\)</span>类似逻辑回归中的<span class="math inline">\(\ln{p \over {1-p}}\)</span>。</p>
<p>这时候，第t棵决策树<span class="math inline">\(f_t(x)\)</span>需要拟合的值就是：
<span class="math display">\[
\begin{aligned}
r_i &amp; = \gamma\ ({y_i \over {1+e^{-\hat y_i^{(t-1)}}}}e^{- \hat y_i^{(t-1)} } - {{1-y_i} \over {1+e^{\hat y_i^{(t-1)}}}} e^{\hat y_i^{(t-1)}}) \\
 &amp; = \gamma\ (y_i - {1 \over {1 + e^{-\hat y_i^{(t-1)}}}}) \\
 &amp; = \gamma\ (y - p_i^{(t-1)})
\end{aligned}
\]</span>
也就是说，当步长（学习率）为1时，拟合的是前t-1棵决策树模型预测的概率之和与真实标签之差。</p>
</div>
<div id="多分类问题下的gbdt" class="section level2">
<h2>多分类问题下的GBDT</h2>
<hr />
<p>有空再写</p>
</div>
