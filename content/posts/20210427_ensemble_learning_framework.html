---
title: 集成学习框架
author: Fang Yongchao
date: "2021-04-27"
output:
  blogdown::html_page:
    toc: yes
    toc_depth: 2
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>

<div id="TOC">
<ul>
<li><a href="#bagging">Bagging</a></li>
<li><a href="#boosting">Boosting</a></li>
</ul>
</div>

<div id="bagging" class="section level2">
<h2>Bagging</h2>
<hr />
<p>Bagging，全称为Bootstrap Aggregating，是对训练集进行有放回的抽样得到其子集，然后每个基学习器使用不同的子集进行训练得到预测结果，最终基于所有的基学习器的预测结果进行投票，从而得到集成模型的预测结果。Bagging中的各个基学习器是相互独立的。</p>
<p>随机森林模型即为最常见的Bagging模型，它是由多个决策树基学习器集成得到的集成模型。</p>
</div>
<div id="boosting" class="section level2">
<h2>Boosting</h2>
<hr />
<p>Boosting的训练过程是有序的，每个基学习器会基于前一个基学习器的结果进行学习，最终基于所有的基学习器的预测值计算得到集成模型的预测结果。</p>
<p>常见的Boosting模型有：</p>
<div id="adaboost" class="section level3">
<h3>Adaboost</h3>
<p>Adaboost，全称为Adaptive boosting，其基本思想为前一个基学习器中分错的样本的权重会得到加强，在权重进行加强后基于所有样本继续训练新的基学习器。循环上述过程直到错误率低于某个值或循环次数达到某个值时停止。</p>
</div>
<div id="gbdt" class="section level3">
<h3>GBDT</h3>
<p>GBDT，全称为Gradient Boosting Decision Tree，其基本思想为以决策树作为基学习器，后一个基学习器以前一个基学习器的残差作为目标值进行训练，最终将所有基学习器的预测值相加作为集成学习模型的预测值。</p>
<p>目前GBDT的主流的工程实现有XGBoost和LightGBM。</p>
</div>
</div>
