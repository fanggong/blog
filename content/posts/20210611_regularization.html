---
title: 参数正则化
author: Fang Yongchao
date: "2021-06-11"
categories:
  - "机器学习"
output:
  blogdown::html_page:
    toc: yes
    toc_depth: 2
---


<div id="TOC">
<ul>
<li><a href="#线性回归的正则化" id="toc-线性回归的正则化">线性回归的正则化</a></li>
<li><a href="#关于l2参数正则化的理论分析" id="toc-关于l2参数正则化的理论分析">关于L2参数正则化的理论分析</a></li>
<li><a href="#关于l1参数正则化的理论分析" id="toc-关于l1参数正则化的理论分析">关于L1参数正则化的理论分析</a></li>
<li><a href="#l2正则化和l1正则化的比较" id="toc-l2正则化和l1正则化的比较">L2正则化和L1正则化的比较</a></li>
</ul>
</div>

<p>以下内容主要参考自<a href="https://scikit-learn.org/stable/user_guide.html">scikit-learn文档</a>和<strong>Deep Learning</strong>一书。</p>
<div id="线性回归的正则化" class="section level2">
<h2>线性回归的正则化</h2>
<hr />
<p>对于参数正则化最早的接触来自于线性回归。</p>
<p><strong>Lasso回归</strong>使用L1范数进行参数正则化，即回归的目标函数为：
<span class="math display">\[
{1 \over 2N }\left \| X\omega-y \right \|_2^2 + \alpha \left \| \omega \right \|_1
\]</span></p>
<p><strong>岭回归</strong>使用L2范数进行参数正则化，即回归的目标函数为：
<span class="math display">\[
\left \| X\omega-y \right \|_2^2 + \alpha \left \| \omega \right \|_2^2
\]</span></p>
<p><strong>Elasti-Net回归</strong>的正则项结合了L1范数和L2范数，其目标函数为：
<span class="math display">\[
{1 \over 2N }\left \| X\omega-y \right \|_2^2 + \alpha \rho \left \| \omega \right \|_1 + {\alpha (1-\rho) \over 2} \left \| \omega \right \|_2^2
\]</span></p>
</div>
<div id="关于l2参数正则化的理论分析" class="section level2">
<h2>关于L2参数正则化的理论分析</h2>
<hr />
<p>L2正则化下的目标函数可以表示为：
<span class="math display">\[
\tilde J(\omega; X, y) = {\alpha \over 2}\omega^T\omega + J(\omega; X, y)
\]</span>
上式的右侧两项分别为正则项和损失函数。</p>
<p>设<span class="math inline">\(\omega^*\)</span>为使得损失函数<span class="math inline">\(J(\omega; X, y)\)</span>取得最小值时的<span class="math inline">\(\omega\)</span>，那么在<span class="math inline">\(\omega^*\)</span>的附近对损失函数进行二次泰勒展开，则有：
<span class="math display">\[
J(\omega) = J(\omega^*) + {1 \over 2}(\omega - \omega^*)^T H (\omega - \omega^*)
\]</span>
其中<span class="math inline">\(H\)</span>是<span class="math inline">\(J\)</span>在<span class="math inline">\(\omega^*\)</span>处的关于<span class="math inline">\(\omega\)</span>的Hessian矩阵。</p>
<p><strong>这里由于<span class="math inline">\(\omega^*\)</span>为损失函数的最优解，损失函数在该点的一阶导为0，故上式的泰勒展开中不含一阶项。</strong></p>
<p>显然，上面的泰勒展开在取到最小值时有：
<span class="math display">\[
J&#39;(\omega) = H(\omega - \omega^*) = 0
\]</span></p>
<p>设<span class="math inline">\(\tilde \omega\)</span>为使得目标函数<span class="math inline">\(\tilde J(\omega; X, y)\)</span>取得最小值时的<span class="math inline">\(\omega\)</span>，那么<span class="math inline">\(\tilde \omega\)</span>应该满足：
<span class="math display">\[
\alpha \tilde \omega + H(\tilde \omega - \omega^*) = 0 \\
(H + \alpha I)\tilde \omega - H\omega^* = 0 \\
\tilde \omega = (H + \alpha I)^{-1} H \omega^*
\]</span>
<strong>到这一步其实已经可以看出来<span class="math inline">\(\alpha\)</span>的作用是在对使得损失函数取得最小值的<span class="math inline">\(\omega = \omega^*\)</span>进行一个缩放了，而且<span class="math inline">\(\alpha\)</span>越大，这个缩放的程度也会越大。</strong></p>
<p>但是为了理解正则项是如何对<span class="math inline">\(\omega^*\)</span>进行缩放的话，需要对上式进行一个变换，这里要用到下述定理：</p>
<ul>
<li>任意的NxN实对称矩阵的特征值都是实数且都有N个线性无关的特征向量。并且这些特征向量都可以正交单位化而得到一组正交且模为1的向量。故实对称矩阵<span class="math inline">\(A\)</span>可被分解成：<br />
<span class="math display">\[
A = Q \Lambda Q^{-1} = Q \Lambda Q^T
\]</span>
其中<span class="math inline">\(Q\)</span>为正交矩阵，<span class="math inline">\(\Lambda\)</span>为实对角矩阵。</li>
</ul>
<p>由于<span class="math inline">\(H\)</span>是实对称矩阵，运用上面的定理，可以得到：
<span class="math display">\[
\begin{aligned}
\tilde \omega &amp; =  (H + \alpha I)^{-1} H \omega^* \\
&amp; = (Q \Lambda Q^T + \alpha I)^{-1} Q \Lambda Q^T \omega^* \\
&amp; = [Q (\Lambda + \alpha I) Q^T]^{-1} Q \Lambda Q^T \omega^* \\
&amp; = Q (\Lambda + \alpha I)^{-1} \Lambda Q^T \omega^*
\end{aligned}
\]</span>
于是可以看到，<strong>权重衰减的效果是沿着由<span class="math inline">\(H\)</span>的特征向量所定义的轴缩放<span class="math inline">\(\omega^*\)</span>，具体来说，会根据<span class="math inline">\(\lambda_i \over {\lambda_i + \alpha}\)</span>因子缩放与<span class="math inline">\(H\)</span>第<span class="math inline">\(i\)</span>个特征向量对齐的<span class="math inline">\(\omega^*\)</span>的分量</strong>。</p>
<p>这个结论是<strong>Deep Learning</strong>书中的原话，但是个人依然觉得实在是很**的抽象，无法与前面的公式联系起来。直到发现做一个下面的变换就理解了：
<span class="math display">\[
Q^T \tilde \omega = (\Lambda + \alpha I)^{-1} \Lambda Q^T \omega^*
\]</span></p>
<p>即<strong>计算<span class="math inline">\(\omega^*\)</span>在<span class="math inline">\(H\)</span>的特征向量所定义的轴上的坐标（<span class="math inline">\(Q^T \omega^*\)</span>），然后使用因子<span class="math inline">\(\lambda_i \over {\lambda_i + \alpha}\)</span>（<span class="math inline">\((\Lambda + \alpha I)^{-1} \Lambda\)</span>）进行缩放。而<span class="math inline">\(\tilde \omega\)</span>在<span class="math inline">\(H\)</span>的特征向量定义的轴上的坐标（<span class="math inline">\(Q^T \tilde \omega\)</span>）应该等于上一步缩放后的结果</strong>。</p>
<p>很显然，对于某确定的<span class="math inline">\(\alpha\)</span>，<span class="math inline">\(\lambda_i\)</span>越小，缩放因子越接近0，从而处于该特征值对应特征向量方向上的<span class="math inline">\(\omega^*\)</span>的分量就会被缩放到很小。</p>
<p>换一个角度考虑，<span class="math inline">\(\lambda_i\)</span>越小，说明当<span class="math inline">\(\omega\)</span>在<span class="math inline">\(\lambda_i\)</span>对应特征向量方向上移动的时候，损失函数变化相对较小（Hessian矩阵的性质），故<span class="math inline">\(\omega^*\)</span>会在较小的特征值对应的特征向量方向上进行更大的移动（即缩放）。</p>
</div>
<div id="关于l1参数正则化的理论分析" class="section level2">
<h2>关于L1参数正则化的理论分析</h2>
<hr />
<p>L1正则化下的目标函数可以表示为：
<span class="math display">\[
\tilde J(\omega; X, y) = \alpha \left \| \omega \right \| _1 + J(\omega; X, y)
\]</span>
同样的，设<span class="math inline">\(\omega^*\)</span>为损失函数取到最小值时的<span class="math inline">\(\omega\)</span>，<span class="math inline">\(\tilde \omega\)</span>为目标函数取到最小值时的<span class="math inline">\(\omega\)</span>，那么<span class="math inline">\(\omega^*\)</span>和<span class="math inline">\(\tilde \omega\)</span>的关系可以由下式得出：
<span class="math display">\[
\tilde J&#39;(\tilde \omega; X, y) = \alpha sign(\tilde \omega) + H(\tilde \omega - \omega^*) = 0
\]</span>
然而，根据上式，我们无法得到一个用<span class="math inline">\(\omega^*\)</span>表示<span class="math inline">\(\tilde \omega\)</span>的直接清晰的代数表达式。因此这里做一个简化假设<span class="math inline">\(H\)</span>是对角矩阵，即<span class="math inline">\(H=diag([H_{1,1}, H_{2,2},...,H_{n,n}])\)</span>，其中每个<span class="math inline">\(H_{i,i}&gt;0\)</span>（该假设可以通过对输入特征进行预处理得到，如PCA）。</p>
<p>那么，在该假设下，目标函数可以表示成如下形式：
<span class="math display">\[
\tilde J(\omega; X, y) = J(\omega^*; X, y) + \sum_i \left[ {1 \over 2} H_{i,i}(\omega_i - \omega_i^*)^2 + \alpha |\omega_i| \right]
\]</span>
为最小化目标函数，可以求得<span class="math inline">\(\omega_i\)</span>有下列形式的解析解（完全不知道这个解析解怎么求出来的，为什么写书的总喜欢在一些不显然的地方很显然…）：
<span class="math display">\[
\omega_i = sign (\omega^*)\max \left \{ |\omega_i^*| - {\alpha \over H_{i,i}},\ 0 \right \}
\]</span>
分析上面的解析解，可以发现：</p>
<ul>
<li><span class="math inline">\(0 &lt; |\omega_i^*| &lt; {\alpha \over H_{i,i}}\)</span>时，<span class="math inline">\(\omega_i\)</span>为0。即<span class="math inline">\(\omega_i\)</span>对损失函数的贡献被正则项抵消，<span class="math inline">\(\omega_i\)</span>被推为0</li>
<li><span class="math inline">\(|\omega_i^*| &gt; {\alpha \over H_{i,i}}\)</span>时，<span class="math inline">\(\omega_i\)</span>会被从<span class="math inline">\(\omega_i^*\)</span>开始向0推动<span class="math inline">\(\alpha \over H_{i,i}\)</span>的距离</li>
</ul>
</div>
<div id="l2正则化和l1正则化的比较" class="section level2">
<h2>L2正则化和L1正则化的比较</h2>
<hr />
<p>通过前面的理论分析就可以得到对L2正则化和L1正则化的几个常识：</p>
<ol style="list-style-type: decimal">
<li>L2正则化中，参数只是在进行缩放，即最小化损失函数得到的参数不为0的话，最小化目标函数得到的参数也不会为0。因此L2正则化不会使参数变得稀疏；</li>
<li>L1正则化中，参数是在被向0推动，当<span class="math inline">\(\alpha\)</span>足够大时，部分特征的参数会被推为0，因此L1正则化也常被用来作为特征选择的工具。</li>
</ol>
</div>
