---
title: adaboost原理
author: Fang Yongchao
date: "2021-05-30"
categories:
  - "机器学习"
output:
  blogdown::html_page:
    toc: yes
    toc_depth: 2
draft: true
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#adaboost算法原理">AdaBoost算法原理</a></li>
<li><a href="#关于样本权重的求解">关于样本权重的求解</a></li>
<li><a href="#回归问题下的adaboost">回归问题下的AdaBoost</a></li>
<li><a href="#adaboost的正则化">AdaBoost的正则化</a></li>
<li><a href="#关于adaboost的几点思考">关于AdaBoost的几点思考</a></li>
</ul>
</div>

<div id="adaboost算法原理" class="section level2">
<h2>AdaBoost算法原理</h2>
<hr />
<p>AdaBoost的基本思想是通过不断修改训练样本的权重来训练新的基学习器，最后将所有基学习器进行加权求和从而得到一个集成模型。即，AdaBoost可以表示为基学习器的加法模型：
<span class="math display">\[
\hat y_i = \sum_{k=1}^K \alpha_k G_k(x_i)
\]</span>
其中<span class="math inline">\(G_k\)</span>为第<span class="math inline">\(k\)</span>个基学习器，<span class="math inline">\(\alpha_k\)</span>为其系数。</p>
<p>以二分类模型为例，将目标变量以±1的形式表示，损失函数定义为指数损失函数，即：
<span class="math display">\[
l(y, \hat y) = e^{-y \hat y}
\]</span></p>
<p>设第t个基分类器为<span class="math inline">\(G_t(x)\)</span>，那么有：
<span class="math display">\[
\hat y^{(t)}_i = \hat y^{(t-1)}_i + \alpha_t G_t(x_i)
\]</span></p>
<p>其目标函数可以表示为：
<span class="math display">\[
\begin{aligned}
obj^{(t)} &amp; = \sum_{i=1}^N e^{-y_i \hat y^{(t-1)}_i -y_i\alpha_tG_t(x_i)} \\
 &amp; = \sum_{i=1}^N \omega_i^{(t)} e^{-y_i \alpha_tG_t(x_i)}  
\end{aligned}
\]</span>
其中<span class="math inline">\(\omega_i^{(t)} = e^{-y_i \hat y^{(t-1)}_i}\)</span>是一个常数可以理解为前t-1个基分类器的预测值带入损失函数得到的值。</p>
<p>观察上面的目标函数可以发现，对于任意的大于0的<span class="math inline">\(\alpha_t\)</span>，当<span class="math inline">\(y_i = G_t(x_i)\)</span>时，<span class="math inline">\(\omega_i^{(t)}\)</span>会被乘上一个小于1的值（即<span class="math inline">\(e^{-\alpha_t}\)</span>），当<span class="math inline">\(y_i \ne G_t(x_i)\)</span>时，<span class="math inline">\(\omega_i^{(t)}\)</span>会被乘上一个大于1的值（即<span class="math inline">\(e^{\alpha_t}\)</span>）。</p>
<p>因此，第t个基分类器<span class="math inline">\(G_t\)</span>的目标函数可以转换为：
<span class="math display">\[
obj^{(t)} = \sum_{y_i \ne G_t(x_i)}\omega_i^{(t)}
\]</span>
这个形式通俗的描述出来就是，<strong>对于前t-1个基分类器都预测错误的样本（<span class="math inline">\(\omega_i^{(t)}\)</span>较大），在第t个基分类器中就不要再预测错了（不要再让该样本进入<span class="math inline">\(y_i \ne G_t(x_i)\)</span>这个集合了）</strong>。</p>
<p>接下来需要对<span class="math inline">\(G_t\)</span>的系数<span class="math inline">\(\alpha_t\)</span>进行求解。对<span class="math inline">\(obj^{(t)} = \sum_{i=1}^N \omega_i^{(t)} e^{-y_i \alpha_tG_t(x_i)}\)</span>关于<span class="math inline">\(\alpha_t\)</span>求导并令其导数等于0，有：
<span class="math display">\[
\sum_{y_i \ne G_t(x_i)} \omega^{(t)}_i e^{\alpha_t} + \sum_{y_i = G_t(x_i)} \omega^{(t)}_i e^{-\alpha_t} = 0 
\]</span>
于是可以得到：
<span class="math display">\[
\alpha_t = {1 \over 2} \log {{\sum_{y_i = G_t(x_i)} \omega^{(t)}_i} \over {\sum_{y_i \ne G_t(x_i)} \omega^{(t)}_i}}
\]</span>
这里令：
<span class="math display">\[
e_t = {\sum_{y_i \ne G_t(x_i)} \omega^{(t)}_i \over \sum_{i=1}^N \omega_i^{(t)}}
\]</span>
则<span class="math inline">\(\alpha_t\)</span>可以写成下面很简洁的形式：
<span class="math display">\[
\alpha_t = {1 \over 2} \log {{ 1- e_t} \over e_t}
\]</span>
这里的<span class="math inline">\(e_t\)</span>我们称之为<strong>分类误差率</strong>。</p>
</div>
<div id="关于样本权重的求解" class="section level2">
<h2>关于样本权重的求解</h2>
<hr />
<p>上面描述了二分类问题下的AdaBoost算法的推导过程，可以发现在整个过程中最重要的就是<span class="math inline">\(\omega_i^{(t)}\)</span>（即样本权重）的求解。它可以理解为前t-1个基分类器的集成模型的预测结果带入损失函数得到的值。</p>
<p>但是在实际的工程化过程中，并不需要每次生成新的基分类器时都去计算前面所有的基分类器得到的集成模型的预测值（这个很明显计算量非常之大）。可以发现：
<span class="math display">\[
\begin{aligned}
\omega_i^{(t)} &amp; = e^{-y_i \hat y^{(t-1)}_i} \\
 &amp; = e^{-y_i (\hat y_i^{(t-2)} + \alpha_{t-1}G_{t-1}(x_i))} \\
 &amp; = \omega_i^{(t-1)} e^{-y_i\alpha_{t-1}G_{t-1}(x_i)}
\end{aligned}
\]</span>
即在生成第t个基分类器时使用的样本权重与前t-2个基分类器并无关系，需要做的只是根据第t-1个基分类器的预测值更新第t-1个基分类器中使用的样本权重即可。</p>
<p><strong>大部分的资料在这里的右侧的式子中会加上一个作为规范化因子的分母，目的是让<span class="math inline">\(\sum_{i=1}^N\omega_i^{(t)}\)</span>之和为1。同时，规范化后的样本权重在计算分类误差率<span class="math inline">\(e_t\)</span>时只需要求解<span class="math inline">\(\sum_{y_i \ne G_t(x_i)} \omega_i^{(t)}\)</span>即可。</strong></p>
</div>
<div id="回归问题下的adaboost" class="section level2">
<h2>回归问题下的AdaBoost</h2>
<hr />
<p>关于回归问题下的AdaBoost，找了很久也没有找到算法的推导过程的相关资料，只能通过<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.31.314&amp;rep=rep1&amp;type=pdf">Improving Regressors using Boosting Techniques</a>这篇文章看到该算法最终的实现方式，总结在这里。</p>
<p>由于回归问题和分类问题下的AdaBoost虽然基本思想是一致的，但是在细节的处理上感觉已经不是一个东西了，所以这里延用论文中使用的数学符号，以免和上面描述的分类问题的AdaBoost混淆。</p>
<div id="adaboost回归问题算法" class="section level3">
<h3>AdaBoost回归问题算法</h3>
<p>对于<span class="math inline">\(N\)</span>个样本的训练集，初始化所有样本的权重为1，即对于任意<span class="math inline">\(i\)</span>，有样本权重<span class="math inline">\(\omega_i = 1\)</span>，之后循环下面的步骤直到平均损失<span class="math inline">\(\bar L\)</span>大于0.5（<strong>这个约束很重要</strong>）。</p>
<ol style="list-style-type: decimal">
<li><p>通过样本权重计算样本被抽样概率<span class="math inline">\(p_i = {\omega_i\ /\ {\sum_{i=1}^N \omega_i}}\)</span>。这其实就是一个将样本权重规范化的步骤，<span class="math inline">\(p_i\)</span>决定了在基学习器下样本的预测损失以什么样的比例计算到总的损失当中；</p></li>
<li><p>构建基学习器<span class="math inline">\(h_t\)</span>，并得到在该学习器下的各样本的预测值<span class="math inline">\(y_t^{(p)}(x_i)\)</span>；</p></li>
<li><p>对于每一个样本，计算其损失：
<span class="math display">\[
L_i = L\left[ |y_i -  y_t^{(p)}(x_i)| \right]
\]</span>
这里我们要计算的实际是相对损失，即令<span class="math inline">\(D\)</span>为所有样本中损失最大值：
<span class="math display">\[
D = \sup |y_i -  y_t^{(p)}(x_i)|,\quad i=1,2,...,N
\]</span>
那么<span class="math inline">\(L_i\)</span>可以有下面三种形式：
<span class="math display">\[
\begin{aligned}
&amp; L_i = {|y_i - y_t^{(p)}(x_i)| \over D}  &amp;  (linear)\\
&amp; L_i = {(y_i - y_t^{(p)}(x_i))^2 \over D^2} &amp; (square)\\
&amp; L_i = 1 - \exp \left({- |y_i -  y_t^{(p)}(x_i)|} \over D \right) &amp; (exponential)
\end{aligned}
\]</span></p></li>
<li><p>计算平均损失：
<span class="math display">\[
\bar L = \sum_{i=1}^N L_i p_i
\]</span></p></li>
<li><p>令<span class="math inline">\(\beta = {\bar L \over {1 - \bar L}}\)</span>。很显然，<span class="math inline">\(\bar L\)</span>越大，则<span class="math inline">\(\beta\)</span>越大，一个小的<span class="math inline">\(\beta\)</span>意味着较小的平均损失，也就是我们对该基学习器有着更强的信心。<strong>由于我们在一开始就有了平均损失<span class="math inline">\(\bar L\)</span>小于0.5的约束，所以这里的<span class="math inline">\(\beta\)</span>必然是一个位于0和1之间的数字。</strong></p></li>
<li><p>对于每一个样本，更新<span class="math inline">\(\omega_i\)</span>为<span class="math inline">\(\omega_i\beta^{1-L_i}\)</span>。<strong>可以理解为，对于基学习器下相对损失<span class="math inline">\(L_i\)</span>小的样本，会通过<span class="math inline">\(\beta^{1-L_i}\)</span>对权重进行一个更大的缩放，有点绕口，简单说就是权重会变得更小。</strong></p></li>
</ol>
<p>当<span class="math inline">\(\bar L\)</span>无法满足一开始的约束（<span class="math inline">\(\bar L&lt;0.5\)</span>）或是循环次数达到我们设定的最大值时结束循环，假设循环进行了<span class="math inline">\(T\)</span>次，那么我们得到了<span class="math inline">\(T\)</span>个基学习器。回归问题下，AdaBoost通过下式得到集成模型的预测值：
<span class="math display">\[
h_f = \inf \left\{ y \in Y:\ \sum_{t:h_t \le y} \log\left({1 \over \beta_t}\right) \ge {1\over2} \sum_t \log\left( {1 \over \beta_t} \right) \right\}
\]</span></p>
<p>这个式子***抽象，解释如下：</p>
<p>对于样本<span class="math inline">\(i\)</span>，<span class="math inline">\(T\)</span>个基学习器可以得到<span class="math inline">\(T\)</span>个预测值，同时有对应的<span class="math inline">\(T\)</span>个关于基学习器的信心的参数<span class="math inline">\(\beta\)</span>。将这<span class="math inline">\(T\)</span>个基学习器得到的预测值<span class="math inline">\(y_i\)</span>按照从小到大排列，即：
<span class="math display">\[
y_i^{(1)} &lt; y_i^{(2)} &lt; \ ...\ &lt; y_i^{(T)}
\]</span>
注意这里的<span class="math inline">\(1,2,...,T\)</span>不是得到基学习器的顺序，而是其预测值从小到大排列后的顺序。</p>
<p>计算<span class="math inline">\(\sum_{i=1}^T\log({1 \over \beta_t})\)</span>，这里个人认为可以理解为：<strong>由于<span class="math inline">\(\beta\)</span>越小说明了我们对该基学习器的信心越强，那么<span class="math inline">\(\log({1 \over \beta})\)</span>和我们对基学习器的信心就是一个正相关的关系，所有的基学习器的<span class="math inline">\(\log({1 \over \beta})\)</span>之和则描述了我们对于集成模型的信心。</strong></p>
<p>然后依次令<span class="math inline">\(t = 1,2,...,T\)</span>计算<span class="math inline">\(\sum_{i=1}^t\log({1 \over \beta_t})\)</span>直到<span class="math inline">\(\sum_{i=1}^t\log({1 \over \beta_t}) \ge {1 \over 2}\sum_{i=1}^T\log({1 \over \beta_t})\)</span>时停止。取此时的<span class="math inline">\(t\)</span>对应的基学习器的预测值<span class="math inline">\(y_i^{(t)}\)</span>作为该样本的预测值。</p>
<p>很显然，当所有基学习器的<span class="math inline">\(\beta\)</span>都相等时，<span class="math inline">\(y_i^{(t)}\)</span>就是所有基学习器预测值的中位数，所以上面的求值公式也被称为<strong>加权中位数</strong>。</p>
</div>
</div>
<div id="adaboost的正则化" class="section level2">
<h2>AdaBoost的正则化</h2>
<hr />
<p>和<a href="../20210507_gbdt">GDBT</a>一样，AdaBoost也是通过在基学习器的系数中加上学习率从而达到正则化的目的。</p>
</div>
<div id="关于adaboost的几点思考" class="section level2">
<h2>关于AdaBoost的几点思考</h2>
<hr />
<p><strong>纯粹个人思考，没有任何依据…</strong></p>
<ol style="list-style-type: decimal">
<li><strong>回归问题下的AdaBoost算法中，最后一步为什么不直接对所有基学习器的预测值取加权平均？</strong><br />
感觉使用加权中位数能最大降低离群值的影响。<br />
假设现在有一个离群样本，所有的基学习器都无法对它作出较好的预测，导致该样本的样本权重会越来越大，那么最终会形成一个局面就是，某一个基学习器是为该样本量身打造的。同时，由于其它样本的样本权重几乎接近于0，导致在该基学习器下的<span class="math inline">\(\bar L\)</span>会非常之小，这也就意味着集成模型会给该基学习器很强的信心（<span class="math inline">\(\log({1 \over \beta})\)</span>）。<br />
这时使用加权平均的话，该离群值会对结果产生很大的影响，但是使用加权中位数的话，就几乎没有影响了。</li>
</ol>
</div>
