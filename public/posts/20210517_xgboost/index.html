<!DOCTYPE html>
<html lang="ch">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="author" content="Fang Yongchao">
    <meta name="description" content="http://kendo.sc.cn">
    <meta name="keywords" content="blog, data science">

    <meta property="og:site_name" content="Connecting the dots">
    <meta property="og:title" content="
  xgboost原理 - Connecting the dots
">
    <meta property="og:description" content="">
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://kendo.sc.cn/posts/20210517_xgboost/">
    <meta property="og:image" content="http://kendo.sc.cn">

    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="http://kendo.sc.cn/posts/20210517_xgboost/">
    <meta name="twitter:image" content="http://kendo.sc.cn">
    
    <meta name="google-site-verification" content="Rn4HeSdMtsEZ8JGAVqKw6WgCPedwLdcfHofZ_XCAZNc" />
    
    <base href="http://kendo.sc.cn/posts/20210517_xgboost/">

    <title>
  xgboost原理 - Connecting the dots
</title>

    <link rel="canonical" href="http://kendo.sc.cn/posts/20210517_xgboost/">
    
    
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.3/css/all.css" integrity="sha384-SZXxX4whJ79/gErwcOYf+zWLeJdY/qpuqC4cAa9rOGUstPomtqpuNWT9wdPEn2fk" crossorigin="anonymous">
    
    
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Liu+Jian+Mao+Cao:400"> 
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Mono:400,700">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400">

    <link rel="stylesheet" href="/css/normalize.min.css">
    <link rel="stylesheet" href="/css/style.min.css">

    

    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css" integrity="sha384-t5CR+zwDAROtph0PXGte6ia8heboACF9R5l/DiY+WZ3P2lxNgvJkQk5n7GPvLMYw" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js" integrity="sha384-FaFLTlohFghEIZkw6VGwmf9ISTubWAVYW8tG8+w2LAIftJEULZABrF9PPFv+tVkH" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/auto-render.min.js" integrity="sha384-bHBqxz8fokvgoJ/sc17HODNxa42TlaEhB+w8ZJXTc2nZf1VgEaFZeZvT4Mznfz0v" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
    
    
    

    <link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

    
      <link rel="alternate" href="http://kendo.sc.cn/index.xml" type="application/rss+xml" title="Connecting the dots">
      <link href="http://kendo.sc.cn/index.xml" rel="feed" type="application/rss+xml" title="Connecting the dots" />
    

    <meta name="generator" content="Hugo 0.101.0" />
  </head>

  <body class="">
    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">Connecting the dots</a>
    <input type="checkbox" id="menu-control"/>
    <label class="menu-mobile  float-right " for="menu-control">
      <span class="btn-mobile  float-right ">&#9776;</span>
      <ul class="navigation-list">
        
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="http://kendo.sc.cn/about">About</a>
            </li>
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="http://kendo.sc.cn/posts">Blog</a>
            </li>
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="http://kendo.sc.cn/tools">Tools</a>
            </li>
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="http://kendo.sc.cn/categories">Categories</a>
            </li>
          
        
        
      </ul>
    </label>
  </section>
</nav>


      <div class="content">
        
  <section class="container post">
  
  <article>

    <header>
      <h1 class="title">xgboost原理</h1>
      <h2 class="date">May 17, 2021</h2>
    </header>

    

<div id="TOC">
<ul>
<li><a href="#目标函数损失函数-正则项" id="toc-目标函数损失函数-正则项">目标函数：损失函数 + 正则项</a></li>
<li><a href="#决策树集成" id="toc-决策树集成">决策树集成</a></li>
<li><a href="#xgboost的损失函数" id="toc-xgboost的损失函数">xgboost的损失函数</a></li>
<li><a href="#xgboost的正则项" id="toc-xgboost的正则项">xgboost的正则项</a></li>
<li><a href="#xgboost的目标函数" id="toc-xgboost的目标函数">xgboost的目标函数</a></li>
<li><a href="#寻找最优结构的树" id="toc-寻找最优结构的树">寻找最优结构的树</a></li>
<li><a href="#几个比较纠结的问题" id="toc-几个比较纠结的问题">几个比较纠结的问题</a></li>
</ul>
</div>

<p>参考文章：</p>
<ul>
<li><a href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html">Introduction to Boosted Trees</a></li>
<li><a href="https://arxiv.org/pdf/1603.02754.pdf">XGBoost: A Scalable Tree Boosting System</a></li>
</ul>
<p>以下内容主要是对上面文章内容的整理以及一些个人理解。</p>
<div id="目标函数损失函数-正则项" class="section level2">
<h2>目标函数：损失函数 + 正则项</h2>
<hr />
<p>在机器学习模型中，目标函数一般由损失函数和正则项组成：
<span class="math display">\[
obj(\theta) = L(\theta) + \Omega(\theta)
\]</span></p>
<p>其中<span class="math inline">\(L(\theta)\)</span>为损失函数，在回归模型中，我们常常定义为MSE，即：
<span class="math display">\[
L(\theta) = \sum_i(y_i - \hat y_i)^2
\]</span>
或是在逻辑回归模型中，我们定义为：
<span class="math display">\[
L(\theta) = \sum_i \left[ y_i\ln(1 + e^{-\hat y_i}) + (1 - y_i)\ln(1 + e^{\hat y_i}) \right]
\]</span>
（逻辑回归的损失函数非常直观：当<span class="math inline">\(y_i\)</span>为1时，<span class="math inline">\(\hat y_i\)</span>越大则<span class="math inline">\(L(\theta)\)</span>越小；而当<span class="math inline">\(y_i\)</span>为0时，<span class="math inline">\(\hat y_i\)</span>越小则<span class="math inline">\(L(\theta)\)</span>越小）</p>
</div>
<div id="决策树集成" class="section level2">
<h2>决策树集成</h2>
<hr />
<p>由于仅有一棵决策树的模型一般无法运用到实际中，所以我们会选择使用决策树集成模型，即将多个决策树的弱学习器的结果进行汇总，从而得到一个强学习器来进行预测。</p>
<p>随机森林模型就是由多个决策树的基学习器进行投票来得到集成模型的预测值，它的特点是基学习器之间相互独立，每个基学习器都仅使用部分样本和特征。</p>
<p>与随机森林这样的Bagging集成模型不同，Boosting集成模型中的每个基学习器基于前一个基学习器的结果进行进一步学习，最后将所有基学习器的预测值通过线性组合得到集成模型的预测结果，可以简单表示为：
<span class="math display">\[
\hat y_i = \sum_{k=1}^Kf_k(x_i)
\]</span>
在xgboost中，使用CART(Classification And Regression Tree)作为集成模型的基学习器。</p>
</div>
<div id="xgboost的损失函数" class="section level2">
<h2>xgboost的损失函数</h2>
<hr />
<p>模型训练的过程即为寻找使得目标函数达到最优的参数的过程。</p>
<p>在Boosting集成模型中，直接寻找全局最优意味着需要同时知道每个基学习器的状态，这对于将决策树作为基学习器的集成模型来说是很难办到的。因此在xgboost中采取了贪心策略，即在每一步寻找当前最优的树。</p>
<p>于是，xgboost中，目标函数可以表示为：
<span class="math display">\[
\begin{aligned}
obj^{(t)} &amp; = \sum_{i=1}^nl(y_i,\ \hat y_i^{(t)}) + \sum_{i=1}^t \Omega(f_i) \\
&amp; = \sum_{i=1}^nl(y_i,\ \hat y_i^{(t-1)} + f_t(x_i)) + \Omega(f_t) + constant
\end{aligned}
\]</span>
即在前t-1棵树的基础上，寻找使得目标函数最小的第t棵树。这里的<span class="math inline">\(constant\)</span>表示的是前t-1棵树对应的正则项。</p>
<p>当使用MSE作为损失函数时，上式可以表示为：
<span class="math display">\[
\begin{aligned}
obj^{(t)} &amp; = \sum_{i=1}^nl(y_i,\ \hat y_i^{(t)}) + \sum_{i=1}^t \Omega(f_i) \\
&amp; = \sum_{i=1}^n \left[ y_i - (\hat y_i^{(t-1)} + f_t(x_i))\right]^2 + \sum_{i=1}^t \Omega(f_i) \\
&amp; = \sum_{i=1}^n \left[ y_i^2 - 2y_i \hat y_i^{(t-1)} - 2y_if_t(x_i) + (\hat y_i^{(t-1)})^2 + 2\hat y_i^{(t-1)}f_t(x_i) + f^2_t(x_i)\right] + \sum_{i=1}^t \Omega(f_i) \quad (1)\\
&amp; = \sum_{i=1}^n \left[ 2(\hat y_i^{(t-1)} - y_i)f_t(x_i) + f_t^2(x_i)\right] + \Omega(f_t) + constant
\end{aligned}
\]</span></p>
<p>这里的<span class="math inline">\(constant\)</span>包括了前t-1棵树对应的正则项以及(1)式第一项中的常数部分。</p>
<p>在实际情况下，由于损失函数的多种多样，我们需要一个可以通用的展开方式，即下述的泰勒展开式。</p>
<ul>
<li><strong>泰勒定理</strong>：
设<span class="math inline">\(n\)</span>是一个正整数。如果定义在一个包含<span class="math inline">\(a\)</span>的区间上的函数<span class="math inline">\(f\)</span>在<span class="math inline">\(a\)</span>点处<span class="math inline">\(n+1\)</span>次可导，那么对于这个区间上的任意<span class="math inline">\(x\)</span>，都有：
<span class="math display">\[
f(x) = f(a) + {f&#39;(a) \over 1!}(x-a) + {f^{(2)}(a) \over 2!}(x-a)^2 +\ ...\ +{f^{(n)}(a) \over n!}(x-a)^n + R_n(x)
\]</span>
其中的多项式称为函数在<span class="math inline">\(a\)</span>处的泰勒展开式，剩余的<span class="math inline">\(R_{n}(x)\)</span> 是泰勒公式的余项，是<span class="math inline">\((x-a)^{n}\)</span>的高阶无穷小。</li>
</ul>
<p>于是可以将<span class="math inline">\(obj^{(t)}\)</span>作泰勒二阶展开成以下形式：
<span class="math display">\[
\begin{aligned}
obj^{(t)} &amp; = \sum_{i=1}^n l\left( y_i,\ \hat y_i^{(t-1)} + f_t(x_i)\right) + \sum_{i=1}^t \Omega(f_i) \\
&amp; = \sum_{i=1}^n \left[ l(y_i, \hat y_i^{(t-1)}) + g_if_t(x_i) + {1 \over 2} h_i f_t^2(x_i)\right] + \sum_{i=1}^t \Omega(f_i)
\end{aligned}
\]</span>
<strong>对应到上方的泰勒展开式，<span class="math inline">\(f\)</span>就是关于预测值<span class="math inline">\(\hat y\)</span>的损失函数<span class="math inline">\(l\)</span>，这里令泰勒展开式中左侧的<span class="math inline">\(x = \hat y_i^{(t-1)} + f_t(x_i)\)</span>，右侧的<span class="math inline">\(a = \hat y_i^{(t-1)}\)</span>。那么<span class="math inline">\(g_i\)</span>就是对损失函数<span class="math inline">\(l\)</span>求一阶导数后将<span class="math inline">\(\hat y_i^{(t-1)}\)</span>带入得到的值，同理，<span class="math inline">\(h_i\)</span>为对损失函数<span class="math inline">\(l\)</span>求二阶导数后将<span class="math inline">\(\hat y_i^{(t-1)}\)</span>带入得到的值。</strong></p>
<p>将目标函数中的常数移除，就得到了我们对第t棵树进行最优化的目标函数为：
<span class="math display">\[
\sum_{i=1}^n \left[ g_if_t(x_i) + {1 \over 2} h_i f_t^2(x_i)\right] + \Omega(f_t)
\]</span>
将该式与前面以MSE作为损失函数时的目标函数比较，可以发现完美对应上了。</p>
</div>
<div id="xgboost的正则项" class="section level2">
<h2>xgboost的正则项</h2>
<hr />
<p>这一部分在文章中没有详细地说明细节，只是给出了一个结论——this one works well in practice…</p>
<p>首先，我们将CART<span class="math inline">\(f(x)\)</span>定义如下：
<span class="math display">\[
f_t(x) = \omega_{q(x)}
\]</span>
这里的<span class="math inline">\(\omega\)</span>是由CART的<span class="math inline">\(T\)</span>个叶子的值组成的一个向量，<span class="math inline">\(q(x)\)</span>是一个映射，该映射将每一个样本映射到<span class="math inline">\(T\)</span>个叶子节点中的某一个。</p>
<p>那么，在xgboost中对正则项的定义为：
<span class="math display">\[
\Omega(f) = \gamma T + {1 \over 2}\lambda \sum_{j=1}^T \omega_j^2
\]</span></p>
</div>
<div id="xgboost的目标函数" class="section level2">
<h2>xgboost的目标函数</h2>
<hr />
<p>在定义了正则项之后，我们可以回到xgboost的目标函数。第t棵树的目标函数可以写作：
<span class="math display">\[
\begin{aligned}
obj^{(t)} &amp; = \sum_{i=1}^n \left[ g_i\omega_{q(x_i)} + {1 \over 2} h_i \omega_{q(x_i)}^2\right] + \gamma T + {1 \over 2}\lambda \sum_{j=1}^T \omega_j^2 \\
&amp; = \sum_{j=1}^T \left[ (\sum_{i \in I_j}g_i)\omega_j + {1 \over 2}(\sum_{i \in I_j}h_i +\lambda)\omega_j^2 \right] + \gamma T
\end{aligned}
\]</span>
<strong>写成这种形式很好理解，因为在同一个叶子节点上的样本的预测值是一样的，所以<span class="math inline">\(n\)</span>个样本的损失函数之和可以分<span class="math inline">\(T\)</span>个叶子节点进行计算。</strong></p>
<p>令<span class="math inline">\(G_j = \sum_{i \in I_j}g_i\)</span>，<span class="math inline">\(H_j = \sum_{i \in I_j}h_i\)</span>，那么目标函数可以写作：
<span class="math display">\[
obj^{(t)} = \sum_{j=1}^T \left[ G_j\omega_j + {1 \over 2}(H_j + \lambda)\omega_j^2 \right] + \gamma T
\]</span>
对于任意的<span class="math inline">\(j\)</span>，上式是一个关于<span class="math inline">\(\omega_j\)</span>的二次多项式，故可以得到当<span class="math inline">\(\omega_j = - {G_j \over H_j + \lambda}\)</span>时，目标函数取到最小值，为：
<span class="math display">\[
- {1 \over 2} \sum_{j=1}^T {G_j^2 \over H_j + \lambda} + \gamma T
\]</span>
这里的<span class="math inline">\(\omega_j,\ j=1,2,3,...,T\)</span>即为第t棵树上每个叶子节点的预测值。</p>
</div>
<div id="寻找最优结构的树" class="section level2">
<h2>寻找最优结构的树</h2>
<hr />
<p>有了上方的目标函数后，就可以开始寻找最优结构的树了。很显然，我们无法枚举出所有的树结构然后比较目标函数的大小，因此这里的策略仍然是贪心算法，即每一步仅寻找当前最优的分割方法。可以知道每次分割目标函数减少量为：
<span class="math display">\[
Gain = {1 \over 2}\left[ {G_L^2 \over H_L + \lambda} + {G_R^2 \over H_R + \lambda} + {(G_L + G_R)^2 \over H_L + H_R + \lambda} \right] - \gamma
\]</span>
可以发现当<span class="math inline">\(\gamma\)</span>越大，在分割时我们对目标函数剩余部分的变化的需求也越大，所以对<span class="math inline">\(\gamma\)</span>进行调参可以起到很好的控制树的复杂度防止过拟合的目的。</p>
</div>
<div id="几个比较纠结的问题" class="section level2">
<h2>几个比较纠结的问题</h2>
<hr />
<ol style="list-style-type: decimal">
<li><p><strong>正则项中包括了叶子节点的值（也就是预测值）的平方和，这意味着预测值越大目标函数越大？</strong><br />
仔细想想这样其实是符合Boosting的思想的。Boosting希望使用一组弱学习器进行组合从而得到一个强学习器，那么对于一个同样的真实值，按照Boosting的思想应该是希望每一个弱学习器都使预测值向真实值前进一步，而不是仅凭某一个弱学习器就做出了非常准确的预测，而剩下的弱学习器不做出任何贡献。<br />
另一方面，这也是符合正则化的思想的，如果一个弱学习器对样本进行了非常准确的预测，那么这个弱学习器很大可能仅仅是在训练集上表现良好而已，也就是过拟合了。</p></li>
<li><p><strong>想出这些方法的人都是啥脑子呢？</strong><br />
不知道</p></li>
</ol>
</div>

  </article>

  <br/>

  
  
</section>

      </div>
      
        <footer class="footer">
  <section class="container">
    
      <div class="sns-shares sp-sns-shares">
        
        
        
        
        
      </div>
    

    
      <a href="https://beian.miit.gov.cn/">蜀ICP备2023005640号</a>
    

     © 2023    ·  Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/naro143/hugo-coder-portfolio">CoderPortfolio</a>. 
    

  </section>
</footer>


      
    </main>

    

  <script src="/js/app.js"></script>
  
  </body>
</html>
