---
title: scikit-learn的特征选择相关接口整理
author: Fang Yongchao
date: "2021-06-22"
categories:
  - "机器学习"
  - "Python"
output:
  blogdown::html_page:
    toc: yes
    toc_depth: 2
---

```{r library, echo=FALSE, warning=FALSE, message=FALSE}
library(reticulate)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

**不要重复造轮子。**

## [`sklearn.feature_selection.GenericUnivariateSelect`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.GenericUnivariateSelect.html#sklearn.feature_selection.GenericUnivariateSelect)

---------------------

对每个特征与目标变量求**score_func**的值，根据结果按照**mode**选择满足**param**条件的特征。

**score_func**应该满足接收n个特征变量和1个目标变量，返回一对长度为n的数组分别表示scores和pvalues。sklearn中提供了5个可能高频率使用的**score_func**：

- **chi2**：卡方统计量，适用于多个二分类特征（或是one-hot编码后的多水平分类特征）和分类目标变量；
- **f_classif**：方差分析的F统计量，适用于连续特征和分类目标变量；
- **f_regression**：单变量线性回归的F统计量，适用于连续特征和连续目标变量；
- **mutual_info_classif**：互信息，适用于分类目标变量；
- **mutual_info_regression**：互信息，适用于连续目标变量。

此外提供了5种选择**mode**：

- **percentile**：根据scores选择得分最高的前一定比例的特征；
- **k_best**：根据scores选择得分最高的前k个特征；
- **fpr**：根据pvalues选择低于某值的特征；
- **fdr**：[False discovery rate](https://en.wikipedia.org/wiki/False_discovery_rate)
- **fwe**：[Family-wise error rate](https://en.wikipedia.org/wiki/Family-wise_error_rate)

自定义**score_func**示例如下（并没有什么实际意义）：
```{python}
def my_score_func(X, y):
    return (X.sum(axis=0), X.mean(axis=0))
```

```{python}
from sklearn.datasets import load_breast_cancer
from sklearn.feature_selection import GenericUnivariateSelect, chi2
X, y = load_breast_cancer(return_X_y=True)
print(X.shape)
transformer = GenericUnivariateSelect(my_score_func, mode='k_best', param=10)
X_new = transformer.fit_transform(X, y)
print(X_new.shape)
```

## [`sklearn.feature_selection.SelectFromModel`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel)

---------------------

通过模型**estimator**在fit之后的feature_importances_或coef_属性对特征进行选择。选择特征的数量通过**threshold**和**max_features**共同控制。

比如我们最常见的利用随机森林进行特征选择。

```{python}
from sklearn.datasets import load_iris
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier
X, y = load_iris(return_X_y=True)
print(X.shape)
selector = SelectFromModel(estimator=RandomForestClassifier(), max_features=2)
X_new = selector.fit_transform(X, y)
print(X_new.shape)
```

## [`sklearn.feature_selection.SequentialFeatureSelector`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html#sklearn.feature_selection.SequentialFeatureSelector)

---------------------

通过模型**estimator**按照**direction**进行前向或后向特征选择。其选择标准由**scoring**决定，最终的选择数量由**n_features_to_select**指定。

```{python}
from sklearn.datasets import load_breast_cancer
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.linear_model import LogisticRegression
X, y = load_breast_cancer(return_X_y=True)
print(X.shape)
selector = SequentialFeatureSelector(
  estimator=LogisticRegression(max_iter=200), 
  n_features_to_select=5, 
  direction='forward'
)
X_new = selector.fit_transform(X, y)
print(X_new.shape)
```


## [`sklearn.feature_selection.VarianceThreshold`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold)

---------------------

删除方差小于**threshold**的特征。该方法用`GenericUnivariateSelect`其实也可以很容易实现。

```{python}
from sklearn.datasets import load_breast_cancer
from sklearn.feature_selection import VarianceThreshold
X, _ = load_breast_cancer(return_X_y=True)
print(X.shape)
selector = VarianceThreshold(threshold=0.2)
X_new = selector.fit_transform(X, y)
print(X_new.shape)
```

## [`sklearn.feature_selection.RFE`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE)

---------------------

通过模型**estimator**在fit之后的feature_importances_或coef_属性迭代地对特征进行选择。每一步会移除**step**数量的特征，直到剩余特征的数量满足**n_features_to_select**。

```{python}
from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFE
X, y = load_breast_cancer(return_X_y=True)
print(X.shape)
selector = RFE(estimator=RandomForestClassifier(), step=3, n_features_to_select=10)
X_new = selector.fit_transform(X, y)
print(X_new.shape)
```

