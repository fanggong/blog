<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>机器学习 on Connecting the dots</title>
    <link>/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 机器学习 on Connecting the dots</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ch</language>
    <lastBuildDate>Tue, 22 Jun 2021 00:00:00 +0000</lastBuildDate><atom:link href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>scikit-learn的特征选择相关接口整理</title>
      <link>/posts/20210622_sklearn_feature_selection_api/</link>
      <pubDate>Tue, 22 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/20210622_sklearn_feature_selection_api/</guid>
      <description>sklearn.feature_selection.GenericUnivariateSelect sklearn.feature_selection.SelectFromModel sklearn.feature_selection.SequentialFeatureSelector sklearn.feature_selection.VarianceThreshold sklearn.feature_selection.RFE 不要重复造轮子。
sklearn.feature_selection.GenericUnivariateSelect 对每个特征与目标变量求score_func的值，根据结果按照mode选择满足param条件的特征。
score_func应该满足接收n个特征变量和1个目标变量，返回一对长度为n的数组分别表示scores和pvalues。sklearn中提供了5个可能高频率使用的score_func：
chi2：卡方统计量，适用于多个二分类特征（或是one-hot编码后的多水平分类特征）和分类目标变量； f_classif：方差分析的F统计量，适用于连续特征和分类目标变量； f_regression：单变量线性回归的F统计量，适用于连续特征和连续目标变量； mutual_info_classif：互信息，适用于分类目标变量； mutual_info_regression：互信息，适用于连续目标变量。 此外提供了5种选择mode：
percentile：根据scores选择得分最高的前一定比例的特征； k_best：根据scores选择得分最高的前k个特征； fpr：根据pvalues选择低于某值的特征； fdr：False discovery rate fwe：Family-wise error rate 自定义score_func示例如下（并没有什么实际意义）：
def my_score_func(X, y): return (X.sum(axis=0), X.mean(axis=0)) from sklearn.datasets import load_breast_cancer from sklearn.feature_selection import GenericUnivariateSelect, chi2 X, y = load_breast_cancer(return_X_y=True) print(X.shape) ## (569, 30) transformer = GenericUnivariateSelect(my_score_func, mode=&amp;#39;k_best&amp;#39;, param=10) X_new = transformer.fit_transform(X, y) print(X_new.shape) ## (569, 10) sklearn.feature_selection.SelectFromModel 通过模型estimator在fit之后的feature_importances_或coef_属性对特征进行选择。选择特征的数量通过threshold和max_features共同控制。
比如我们最常见的利用随机森林进行特征选择。
from sklearn.datasets import load_iris from sklearn.</description>
    </item>
    
    <item>
      <title>参数正则化</title>
      <link>/posts/20210611_regularization/</link>
      <pubDate>Fri, 11 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/20210611_regularization/</guid>
      <description>线性回归的正则化 关于L2参数正则化的理论分析 关于L1参数正则化的理论分析 L2正则化和L1正则化的比较 以下内容主要参考自scikit-learn文档和Deep Learning一书。
线性回归的正则化 对于参数正则化最早的接触来自于线性回归。
Lasso回归使用L1范数进行参数正则化，即回归的目标函数为： \[ {1 \over 2N }\left \| X\omega-y \right \|_2^2 + \alpha \left \| \omega \right \|_1 \]
岭回归使用L2范数进行参数正则化，即回归的目标函数为： \[ \left \| X\omega-y \right \|_2^2 + \alpha \left \| \omega \right \|_2^2 \]
Elasti-Net回归的正则项结合了L1范数和L2范数，其目标函数为： \[ {1 \over 2N }\left \| X\omega-y \right \|_2^2 + \alpha \rho \left \| \omega \right \|_1 + {\alpha (1-\rho) \over 2} \left \| \omega \right \|_2^2 \]</description>
    </item>
    
    <item>
      <title>adaboost原理</title>
      <link>/posts/20210530_adaboost/</link>
      <pubDate>Sun, 30 May 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/20210530_adaboost/</guid>
      <description>AdaBoost算法原理 关于样本权重的求解 回归问题下的AdaBoost AdaBoost的正则化 关于AdaBoost的几点思考 AdaBoost算法原理 AdaBoost的基本思想是通过不断修改训练样本的权重来训练新的基学习器，最后将所有基学习器进行加权求和从而得到一个集成模型。即，AdaBoost可以表示为基学习器的加法模型： \[ \hat y_i = \sum_{k=1}^K \alpha_k G_k(x_i) \] 其中\(G_k\)为第\(k\)个基学习器，\(\alpha_k\)为其系数。
以二分类模型为例，将目标变量以±1的形式表示，损失函数定义为指数损失函数，即： \[ l(y, \hat y) = e^{-y \hat y} \]
设第t个基分类器为\(G_t(x)\)，那么有： \[ \hat y^{(t)}_i = \hat y^{(t-1)}_i + \alpha_t G_t(x_i) \]
其目标函数可以表示为： \[ \begin{aligned} obj^{(t)} &amp;amp; = \sum_{i=1}^N e^{-y_i \hat y^{(t-1)}_i -y_i\alpha_tG_t(x_i)} \\ &amp;amp; = \sum_{i=1}^N \omega_i^{(t)} e^{-y_i \alpha_tG_t(x_i)} \end{aligned} \] 其中\(\omega_i^{(t)} = e^{-y_i \hat y^{(t-1)}_i}\)是一个常数可以理解为前t-1个基分类器的预测值带入损失函数得到的值。
观察上面的目标函数可以发现，对于任意的大于0的\(\alpha_t\)，当\(y_i = G_t(x_i)\)时，\(\omega_i^{(t)}\)会被乘上一个小于1的值（即\(e^{-\alpha_t}\)），当\(y_i \ne G_t(x_i)\)时，\(\omega_i^{(t)}\)会被乘上一个大于1的值（即\(e^{\alpha_t}\)）。
因此，第t个基分类器\(G_t\)的目标函数可以转换为： \[ obj^{(t)} = \sum_{y_i \ne G_t(x_i)}\omega_i^{(t)} \] 这个形式通俗的描述出来就是，对于前t-1个基分类器都预测错误的样本（\(\omega_i^{(t)}\)较大），在第t个基分类器中就不要再预测错了（不要再让该样本进入\(y_i \ne G_t(x_i)\)这个集合了）。</description>
    </item>
    
    <item>
      <title>xgboost原理</title>
      <link>/posts/20210517_xgboost/</link>
      <pubDate>Mon, 17 May 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/20210517_xgboost/</guid>
      <description>目标函数：损失函数 + 正则项 决策树集成 xgboost的损失函数 xgboost的正则项 xgboost的目标函数 寻找最优结构的树 几个比较纠结的问题 参考文章：
Introduction to Boosted Trees XGBoost: A Scalable Tree Boosting System 以下内容主要是对上面文章内容的整理以及一些个人理解。
目标函数：损失函数 + 正则项 在机器学习模型中，目标函数一般由损失函数和正则项组成： \[ obj(\theta) = L(\theta) + \Omega(\theta) \]
其中\(L(\theta)\)为损失函数，在回归模型中，我们常常定义为MSE，即： \[ L(\theta) = \sum_i(y_i - \hat y_i)^2 \] 或是在逻辑回归模型中，我们定义为： \[ L(\theta) = \sum_i \left[ y_i\ln(1 + e^{-\hat y_i}) + (1 - y_i)\ln(1 + e^{\hat y_i}) \right] \] （逻辑回归的损失函数非常直观：当\(y_i\)为1时，\(\hat y_i\)越大则\(L(\theta)\)越小；而当\(y_i\)为0时，\(\hat y_i\)越小则\(L(\theta)\)越小）
决策树集成 由于仅有一棵决策树的模型一般无法运用到实际中，所以我们会选择使用决策树集成模型，即将多个决策树的弱学习器的结果进行汇总，从而得到一个强学习器来进行预测。
随机森林模型就是由多个决策树的基学习器进行投票来得到集成模型的预测值，它的特点是基学习器之间相互独立，每个基学习器都仅使用部分样本和特征。
与随机森林这样的Bagging集成模型不同，Boosting集成模型中的每个基学习器基于前一个基学习器的结果进行进一步学习，最后将所有基学习器的预测值通过线性组合得到集成模型的预测结果，可以简单表示为： \[ \hat y_i = \sum_{k=1}^Kf_k(x_i) \] 在xgboost中，使用CART(Classification And Regression Tree)作为集成模型的基学习器。</description>
    </item>
    
    <item>
      <title>梯度提升树（GBDT）原理</title>
      <link>/posts/20210507_gbdt/</link>
      <pubDate>Fri, 07 May 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/20210507_gbdt/</guid>
      <description>GBDT算法原理 回归问题下的GDBT 二分类问题下的GBDT GBDT算法原理 GBDT可以表示为决策树的加法模型，即： \[ \hat y_i = \sum_{k=1}^K f_k(x_i) \] 其中\(f\)表示决策树，\(K\)为决策树的数量。
通过前向分步骤算法，第t步得到的决策树模型为\(f_t(x)\)，有： \[ \hat y_i^{(t)} = \hat y_i^{(t-1)} + f_t(x_i) \] 其目标函数为： \[ obj^{(t)} = \sum_{i=1}^N l\left(y_i, \hat y_i^{(t-1)} + f_t(x_i) \right) \] 将目标函数使用泰勒一阶展开，有： \[ obj^{(t)} = \sum_{i=1}^N \left[ l(y_i, \hat y_i^{(t-1)}) + g_i f_t(x_i) \right] \] 这里的\(g_i = \left[ {\partial\ l(y_i, \hat y_i) \over \partial \hat y_i} \right]_{\hat y_i = \hat y_i^{(t-1)}}\)，即损失函数关于\(\hat y_i\)求导后带入\(\hat y_i = \hat y_i^{(t-1)}\)。</description>
    </item>
    
    <item>
      <title>决策树的特征选择标准</title>
      <link>/posts/20210428_concepts_about_decision_tree/</link>
      <pubDate>Wed, 28 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/20210428_concepts_about_decision_tree/</guid>
      <description>信息熵(Entropy) 条件熵(Conditional Entropy) 信息增益(Information Gain, IG) 信息增益比(Information Gain Ratio) 基尼指数(Gini Index) 信息熵(Entropy) 设\(Y\)是取有限个值的离散随机变量，其概率分布为： \[ P(Y = y_i) = p_i,\quad i = 1, 2, 3, ..., n \]
则随机变量\(Y\)的熵的定义为： \[ H(Y) = -\sum_{i=1}^n p_i\log_bp_i \] 根据对数的底\(b\)的不同，熵的单位不同。当\(b\)为2时，单位为比特(bit)，当\(b\)为e时，单位为纳特(nat)。
熵被认为是不确定性的度量。很显然当系统内只有一个事件且该事件必定发生时，熵取最小值为0。当系统内各事件发生概率相同时，熵取最大值\(\log_bn\)，此时系统内的不确定性最高。特别的，当\(p_i\)为0时，\(p_i \log_bp_i\)为0。
条件熵(Conditional Entropy) 条件熵表示基于某条件下的信息熵。定义为： \[ H(Y|X) = \sum_{i=1}^np_iH(Y|X = x_i) \] 这里的\(p_i=P(X = x_i),\quad i = 1,2,3,...,n\)。\(H(Y|X = x_i)\)为\(X\)取\(x_i\)时，\(Y\)的信息熵。
信息增益(Information Gain, IG) 信息增益等于信息熵减去条件熵。即： \[ IG(Y, X) = H(Y) - H(Y|X) \] 可以理解为在知道变量\(X\)后，\(Y\)的不确定性减少了多少。
决策树的ID3算法中采用信息增益作为特征选择标准。</description>
    </item>
    
    <item>
      <title>随机森林模型优化</title>
      <link>/posts/20210426_optimise_random_forest/</link>
      <pubDate>Mon, 26 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/20210426_optimise_random_forest/</guid>
      <description>森林中决策树的数量 在节点进行分割的判断标准 每棵树的最大深度 在节点进行分割所需要的最小样本数 叶子节点的最大数量 寻找最佳分割时使用的特征数量 使用bootstrap时的抽样数量 原文地址：Random Forest: Hyperparameters and how to fine-tune them
参考链接：sklearn.ensemble.RandomForestClassifier
随机森林模型的可调参数主要包括：
森林中决策树的数量 在节点进行分割的判断标准 每棵树的最大深度 在节点进行分割所需要的最小样本数 叶子节点的最大数量 寻找最佳分割时使用的特征数量 使用bootstrap时的抽样数量 森林中决策树的数量 通过构建一个包含大量决策树的随机森林，可以得到相对低方差的、鲁棒性好的模型，虽然这个模型的构建可能需要花费大量的时间。这里的诀窍在于对数据进行评估：我们有多少的样本，每个样本又有多少的特征可用。
由于随机森林模型的随机性，如果我们的数据中包含大量的特征，那么当决策树过少时会出现一些具有高信息量的特征没有被使用，或者很少被使用的情况。
同样的，对于样本来说，如果数据中包含大量的样本，那么过少的决策树也会导致一些样本没有被使用的情况出现。
随机森林很少会出现过拟合，所以可以通过增加决策树的数量来减少模型的误差，虽然这样会大大增加模型的训练时间。唯一需要注意的是，不需要无脑增加决策树的数量，因为很显然随着决策树数量的增加，单位训练时间提供的误差减少的收益已经不是很高了。
结论：对森林中的决策树的数量进行调优是没有多大意义的，只需要将它设定为一个较大的且计算上可行的数字就可以了。
在节点进行分割的判断标准 决策树通过计算判断每个节点上哪个特征的哪个值能最好地分割这个节点。这个判断的标准，在分类模型中会使用Gini或Entropy，而在回归模型中会使用MAE或MSE。
在回归模型中，如果我们的数据中没有包含大量的离群值的话，一般选择使用MSE，因为MSE会对离群值有更大的惩罚。
在分类模型中，Gini相对Entropy来说计算成本更低，但是很难断言Gini和Entropy哪个更好。
结论：由于不同的判断标准可能会构建完全不同的模型，而且其可选项也只有两种，所以建议两种都试一试吧。
每棵树的最大深度 增加决策树的深度可以增加决策树中包含的特征的信息量。如果是在决策树模型下，这会导致过拟合，但是在随机森林模型中，由于集成的关系，我们可以加大树的深度。
这个参数应该根据特征的数量选择一个合理的数字，然后进行微调。但是在合理区间的微调对于模型的效果的影响微乎其微，所以其实我们在Grid Search的时候可以考虑不对这个参数进行调整。
结论：对每棵树的最大深度进行调优的意义不大，选择一个合理的数字即可。
在节点进行分割所需要的最小样本数 在参考的原文中没有对这个参数进行太多描述，个人理解调整在节点进行分割所需要的最小样本数也只是一种调整单个决策树形态的手段而已，当这个参数很小时同样会导致单个决策树的过拟合，但是放到整个随机森林模型中时该问题也就不存在了。
结论：不需要对在节点进行分割所需要的最小样本数这个参数进行调整。
叶子节点的最大数量 在随机森林模型中这个参数不是很重要，但是在决策树模型中，需要对这个参数进行调整以防止过拟合和提高模型的可解释性。
结论：不需要对叶子节点的最大数量这个参数进行调整。
寻找最佳分割时使用的特征数量 这是随机森林模型调优时最重要的一个参数，最好的办法是通过Grid Search和Cross Validation得到最优参数，这里应该考虑一下几点：
使用较少的特征会降低集成的方差，但是会导致较高的单个决策树的偏差 这个值应该根据我们的数据中高信息量高质量的特征数量来决定。如果数据中的特征都是非常干净高质量的，那么这个参数可以设定得相对较小。但是如果我们的数据中包含大量的噪声，那么这个参数应该设定的大一些以提高高信息高质量的特征被纳入的机会 增加寻找最佳分割时使用的特征数量可以降低模型的偏差，因为好的特征被使用的机会大大增加了。同时，这也会导致模型的方差增大，当然，模型的训练时间也会大大增加 考虑到以上几点，在使用Grid Search对该参数进行调优时，可以尝试一下几个值：
None：使用全部的特征 sqrt：使用全部特征的数量开根号，即如果有25个特征，那么在寻找最佳分割时使用随机的5个特征 0.2：使用全部特征的20%，当然可以把0.3，0.4，0.5…都试一试。对于回归模型来说，0.33是一个好的开始 结论：在对每个节点进行分割时，仔细调整要考虑的特征数量是基本的，因此考虑使用Grid Search寻找最佳参数。
使用bootstrap时的抽样数量 由于bootstrap采用的是有放回的抽样，所以即使抽样数量与样本总量一致，在训练每棵树时使用的数据也是不同的。
袋外数据（Out Of Bag，即进行有放回的抽样时，有大约三分之一多的数据其实是不会被抽到的，这个数据即袋外数据），可以用来作为随机森林的测试集进行模型评估。有点费解，袋外数据应该是相对于每一个决策树存在的，如何作为最终集成的随机森林的测试集。乍一看感觉是有一些反常识的，不知道是否我的理解上有误，或者说这个理论有严格的证明
结论：不需要对使用bootstrap时的抽样数量这个参数进行调整。</description>
    </item>
    
    <item>
      <title>WOE，IV和PSI</title>
      <link>/posts/20210418_concepts_about_feature_selection/</link>
      <pubDate>Sun, 18 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/20210418_concepts_about_feature_selection/</guid>
      <description>WOE(Weight of Evidence)和IV(Information Value) 群体稳定性指标(Population Stability Index, PSI) 这三个概念对我来说并不经常用到，但是每次用到的时候具体细节总是记串了，所以特别记下来。
WOE(Weight of Evidence)和IV(Information Value) WOE为“当前分组下正样本占所有正样本比例”与“当前分组下负样本占所有负样本比例”的对数差，即对包含\(\{ x_1, x_2, ..., x_n \}\)\(n\)个类别的变量\(X\)，有： \[ p_{i1} = {N(Y = 1|X = x_i) \over N(Y = 1)}; \quad p_{i0} = {N(Y = 0|X = x_i) \over N(Y = 0)} \] \[ WOE(x_i) = \ln { p_{i1} \over p_{i0} } \]
可以看出：
当前分组下正负样本比例与总的正负样本比例相同时，WOE的值为0 当前分组下正样本比例高于总的正样本比例时，WOE值为正 当前分组下正样本比例低于总的正样本比例时，WOE值为负 IV为WOE的加权和，其计算方式为：
\[ IV = \sum_i(p_{i1} - p_{i0})*WOE(x_i) \]
群体稳定性指标(Population Stability Index, PSI) PSI的作用是判断变量分布的稳定性，令\(A_i\)和\(E_i\)分别为类别\(i\)在变量中的实际占比和预期占比，那么有： \[ PSI = \sum_i(A_i - E_i)\ln({A_i \over E_i}) \] 很显然，PSI的值越小，变量实际和预期的分布差距越小，即变量的分布越稳定。</description>
    </item>
    
  </channel>
</rss>
