<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>机器学习 on Fang Yongchao</title>
    <link>https://fanggong.pub/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 机器学习 on Fang Yongchao</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ch</language>
    <lastBuildDate>Wed, 28 Apr 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://fanggong.pub/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>决策树的特征选择标准</title>
      <link>https://fanggong.pub/posts/20210428_concepts_about_decision_tree/</link>
      <pubDate>Wed, 28 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://fanggong.pub/posts/20210428_concepts_about_decision_tree/</guid>
      <description>信息熵(Entropy)条件熵(Conditional Entropy)信息增益(Information Gain, IG)信息增益比(Information Gain Ratio)基尼指数(Gini Index)信息熵(Entropy)设\(Y\)是取有限个值的离散随机变量，其概率分布为：\[P(Y = y_i) = p_i,\quad i = 1, 2, 3, ..., n\]
则随机变量\(Y\)的熵的定义为：\[H(Y) = -\sum_{i=1}^n p_i\log_bp_i\]根据对数的底\(b\)的不同，熵的单位不同。当\(b\)为2时，单位为比特(bit)，当\(b\)为e时，单位为纳特(nat)。
熵被认为是不确定性的度量。很显然当系统内只有一个事件且该事件必定发生时，熵取最小值为0。当系统内各事件发生概率相同时，熵取最大值\(\log_bn\)，此时系统内的不确定性最高。特别的，当\(p_i\)为0时，\(p_i \log_bp_i\)为0。
条件熵(Conditional Entropy)条件熵表示基于某条件下的信息熵。定义为：\[H(Y|X) = \sum_{i=1}^np_iH(Y|X = x_i)\]这里的\(p_i=P(X = x_i),\quad i = 1,2,3,...,n\)。\(H(Y|X = x_i)\)为\(X\)取\(x_i\)时，\(Y\)的信息熵。
信息增益(Information Gain, IG)信息增益等于信息熵减去条件熵。即：\[IG(Y, X) = H(Y) - H(Y|X)\]可以理解为在知道变量\(X\)后，\(Y\)的不确定性减少了多少。</description>
    </item>
    
    <item>
      <title>集成学习框架</title>
      <link>https://fanggong.pub/posts/20210427_ensemble_learning_framework/</link>
      <pubDate>Tue, 27 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://fanggong.pub/posts/20210427_ensemble_learning_framework/</guid>
      <description>BaggingBoostingBaggingBagging，全称为Bootstrap Aggregating，是对训练集进行有放回的抽样得到其子集，然后每个基学习器使用不同的子集进行训练得到预测结果，最终基于所有的基学习器的预测结果进行投票，从而得到集成模型的预测结果。Bagging中的各个基学习器是相互独立的。
随机森林模型即为最常见的Bagging模型，它是由多个决策树基学习器集成得到的集成模型。
BoostingBoosting的训练过程是有序的，每个基学习器会基于前一个基学习器的结果进行学习，最终基于所有的基学习器的预测值计算得到集成模型的预测结果。
常见的Boosting模型有：
AdaboostAdaboost，全称为Adaptive boosting，其基本思想为前一个基学习器中分错的样本的权重会得到加强，在权重进行加强后基于所有样本继续训练新的基学习器。循环上述过程直到错误率低于某个值或循环次数达到某个值时停止。
GBDTGBDT，全称为Gradient Boosting Decision Tree，其基本思想为以决策树作为基学习器，后一个基学习器以前一个基学习器的残差作为目标值进行训练，最终将所有基学习器的预测值相加作为集成学习模型的预测值。
目前GBDT的主流的工程实现有XGBoost和LightGBM。
</description>
    </item>
    
    <item>
      <title>WOE，IV和PSI</title>
      <link>https://fanggong.pub/posts/20210418_concepts_about_feature_selection/</link>
      <pubDate>Sun, 18 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://fanggong.pub/posts/20210418_concepts_about_feature_selection/</guid>
      <description>WOE(Weight of Evidence)和IV(Information Value)群体稳定性指标(Population Stability Index, PSI)WOE(Weight of Evidence)和IV(Information Value)WOE为“当前分组下正样本占所有正样本比例”与“当前分组下负样本占所有负样本比例”的对数差，即对包含\(\{ x_1, x_2, ..., x_n \}\)\(n\)个类别的变量\(X\)，有：\[p_{i1} = {N(Y = 1|X = x_i) \over N(Y = 1)}; \quad p_{i0} = {N(Y = 0|X = x_i) \over N(Y = 0)}\]\[WOE(x_i) = \ln { p_{i1} \over p_{i0} } \]
可以看出：
当前分组下正负样本比例与总的正负样本比例相同时，WOE的值为0当前分组下正样本比例高于总的正样本比例时，WOE值为正当前分组下正样本比例低于总的正样本比例时，WOE值为负IV为WOE的加权和，其计算方式为：
\[IV = \sum_i(p_{i1} - p_{i0})*WOE(x_i)\]</description>
    </item>
    
  </channel>
</rss>
